{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------\n",
    "# This program loads (reads) an LR model (built before \n",
    "# by the logistic_regression_builder.py program) and \n",
    "# then predicts new emails as \"spam\" or \"non-spam\".\n",
    "#------------------------------------------------------\n",
    "# Input Parameters:\n",
    "#    argv[0]: String, is the name of the Python program\n",
    "#    argv[1]: String, new emails to be classified [Query Data]\n",
    "#    argv[2]: String, output path for the saved built model\n",
    "#-------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.mllib.feature import HashingTF\n",
    "from pyspark.mllib.classification import LogisticRegressionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#===============================================\n",
    "# Next, we build a simple Python function, `classify()` \n",
    "# to classify new emails with the built LR model:\n",
    "# predict/classify email and return \n",
    "#    0 (for non-spam), and \n",
    "#    1 (for spam)\n",
    "#\n",
    "# Parameters:\n",
    "#\n",
    "#    email: a new email to be classified\n",
    "#    tf: an instance of HashingTF\n",
    "#    model: an LR model\n",
    "#\n",
    "def classify(email, tf, model):\n",
    "    # tokenize an email into words\n",
    "    tokens = email.split()\n",
    "    # create features for a given email\n",
    "    features = tf.transform(tokens)\n",
    "    # classify email into \"spam\" (class 1) or \"non-spam\" (class 0)\t\n",
    "    return model.predict(features)\n",
    "#end-def\n",
    "#==============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"logistic_regression_predictor\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step-1: define input paths\n",
    "saved_model_path =  \"/user/arnavmoutl12edu/module7/model\"\n",
    "# \"/.../model\"\n",
    "new_emails_path = \"/user/arnavmoutl12edu/module7/new_emails.txt\"\n",
    "# \"/.../new_emails.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved_model_path: /user/arnavmoutl12edu/module7/model\n",
      "new_emails_path: /user/arnavmoutl12edu/module7/new_emails.txt\n"
     ]
    }
   ],
   "source": [
    "# check out the inputs\n",
    "print(\"saved_model_path: {}\".format(saved_model_path))\n",
    "print(\"new_emails_path: {}\".format(new_emails_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we load the model from a saved location:\n",
    "LR_model = LogisticRegressionModel.load(spark.sparkContext, saved_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We still need to create a TF to create features \n",
    "# for the query of new emails:\n",
    "FEATURES_HTF = HashingTF(numFeatures=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we can predict new emails:\n",
    "    \n",
    "# build an RDD[String] for the new emails\n",
    "# for new_emails, create RDD[String], where each \n",
    "# element is an email as a String\n",
    "new_emails = spark.sparkContext.textFile(new_emails_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next we classify every new email\n",
    "# classified is an RDD[classification, email], \n",
    "# where classification will be either 0 (for non-spam email)  \n",
    "# or 1 (for spam email)\n",
    "classified = new_emails.map(lambda email: (classify(email, FEATURES_HTF, LR_model), email))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we debug/examine the classified emails:\n",
    "# use collect() if you are doing debugging/testing\n",
    "# of small number of emails\n",
    "predictions = classified.collect()\n",
    "spam_count = 0\n",
    "nospam_count = 0\n",
    "error_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction=1\t query email=this is a year of promotion for Galaxy End of YearPromo You have 1 week remaining to retrieve your won prize for the Samsung Galaxy Xmas Promo 'C' draw category winning prize of Seven Hundred and Fifty Thousand Euros each and a Samsung Galaxy S6 EDGE. Winning Ticket Number:WIN-707-COS.  We advise you to keep this winning notification confidential and away from public notice to avoid double claim/mistransfer or impersonation until after remittance/payment to you.\n",
      "prediction=1\t query email=you are the lucky one: We've picked out 10 new matches for you. Meet them now and then check out all the singles in your area! you might win a prize too\n",
      "prediction=0\t query email=Do not miss your chances: Get Viagra real cheap!  Send money right away to ...\n",
      "prediction=0\t query email=Get real money fast: With my position in the office i assure you with 100% risk free that this transaction is not a childish game play and i want you to indicate your full interest with assurance of trust that you will not betray me once the fund is transfer into your nominated bank account, while i look forward for your urgent reply.\n",
      "prediction=1\t query email=Dear Spark Learner, Thanks so much for attending the Spark Summit 2014!  Check out videos of talks from the summit at ...\n",
      "prediction=0\t query email=Hi Mom, Apologies for being late about emailing and forgetting to send you the package.  I hope you and bro have been ...\n",
      "prediction=0\t query email=Wow, hey Fred, just heard about the Spark petabyte sort.  I think we need to take time to try it out immediately ...\n",
      "prediction=0\t query email=Hi Spark user list, This is my first question to this list, so thanks in advance for your help!  I tried running ...\n",
      "prediction=1\t query email=Please do not reply to this email, as mail sent to this address cannot be answered. If you have questions please visit our Customer Support page and select the applicable contact method. \n",
      "prediction=0\t query email=Hi Arnav, Are you ready to sharpen your Apache Spark skills? This year at Spark. AI Summit, you'll have the opportunity to take a full day of training (June 4th), that includes hands-on exercises and tutorials taught by leading industry experts.\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# predications is a list of pair of (classification, email)\n",
    "# p denotes a pair of (classification, email); \n",
    "# p[0] denotes predicted classification and \n",
    "# p[1] denotes an email\n",
    "for p in predictions: \n",
    "    if p[0] == 0:\n",
    "        nospam_count += 1\n",
    "    elif p[0] == 1:\n",
    "        spam_count += 1\n",
    "    else:\n",
    "        error_count += 1\n",
    "    #\n",
    "    print(\"prediction=\" + str(p[0]) + \"\\t query email=\" + str(p[1]))\n",
    "#end-for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam_count=4\n",
      "nospam_count=6\n",
      "error_count=0\n"
     ]
    }
   ],
   "source": [
    "print(\"spam_count=\" + str(spam_count))\n",
    "print(\"nospam_count=\" + str(nospam_count))\n",
    "print(\"error_count=\" + str(error_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
